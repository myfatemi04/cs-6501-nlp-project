{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59554e51-d6eb-4218-bcfa-2f7bd2d835b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469678bf-d4fe-4f0e-88f8-a18c6bd8bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41801a32-8304-4e60-8365-101c680149bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f902b10ba8294b0184de81dc74b142ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\",\n",
    ").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85929d39-5f56-45f2-8f87-b542434ec837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"wikitext103-v1-filtered.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6a1286-6ee1-4b04-b891-0006673ae693",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_padding = False\n",
    "\n",
    "if add_padding:\n",
    "    # https://discuss.huggingface.co/t/llama2-pad-token-for-batched-inference/48020\n",
    "    # add_special_tokens doesn't work for some reason\n",
    "    # tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Mini-test\n",
    "    def mini_test():\n",
    "        tokenization = tokenizer(texts[:1], padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "        sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "        print(sequence_lengths)\n",
    "        # print(tokenization.input_ids[0, -sequence_lengths[0]:])\n",
    "        # print(tokenizer.decode(tokenization.input_ids[0]))\n",
    "        # assert (tokenization.input_ids[0, -sequence_lengths[0]:] > 0).all()\n",
    "    \n",
    "    mini_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e315ad3f-dc26-438a-9527-145764cf0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of models / optimizers\n",
    "\n",
    "import autoencoder\n",
    "\n",
    "cfg = {\n",
    "    \"act_size\": 4096,\n",
    "    \"dict_size\": 4096 * 16,\n",
    "    \"enc_dtype\": \"bf16\",\n",
    "    \"l1_coeff\": 3e-3,\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "# might eventually have one model per layer\n",
    "model = autoencoder.AutoEncoder(cfg=cfg)\n",
    "optim = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32691ce-5e24-491f-b02f-b0b84570824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   0%|          | 738/749962 [03:04<51:56:53,  4.01it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print((len(hidden_states), *hidden_states[0].shape))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# (33, 16, <max_sequence_length>, 4096)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# hidden_states = [h.cpu() for h in hidden_states]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#         hidden_state.append(hidden_states[layer_i][text_i, -sequence_lengths[text_i]:, :])\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#     hidden_states_chunk.append(tuple(hidden_state))\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     hidden_states_chunk\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Store the chunk.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(hidden_states_chunk, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def run():\n",
    "    hidden_states_chunk = []\n",
    "    index = 0\n",
    "    # batch_size = 1\n",
    "    chunk_id = 0\n",
    "    counter = 0\n",
    "    with tqdm.tqdm(desc='Running inference', total=len(texts)) as pbar:\n",
    "        while index < len(texts):\n",
    "            # batch = texts[index:index + batch_size]\n",
    "            tokenization = tokenizer(\n",
    "                texts[index],\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=True,\n",
    "                padding=False,\n",
    "            ).to('cuda')\n",
    "            sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(\n",
    "                    **tokenization,\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                hidden_states = outputs.hidden_states\n",
    "                \n",
    "                # print((len(hidden_states), *hidden_states[0].shape))\n",
    "                # (33, 16, <max_sequence_length>, 4096)\n",
    "                # hidden_states = [h.cpu() for h in hidden_states]\n",
    "                # for text_i in range(len(batch)):\n",
    "                #     hidden_state = []\n",
    "                #     for layer_i in range(len(hidden_states)):\n",
    "                #         hidden_state.append(hidden_states[layer_i][text_i, -sequence_lengths[text_i]:, :])\n",
    "                #     hidden_states_chunk.append(tuple(hidden_state))\n",
    "                hidden_states_chunk.append(torch.stack(hidden_states, dim=0).cpu())\n",
    "            \n",
    "            # index += batch_size\n",
    "            # pbar.update(len(batch))\n",
    "            index += 1\n",
    "            pbar.update()\n",
    "            \n",
    "            # (1 + n_layers) tuple of tensors [batch_size, sequence_length, d_model=4096]\n",
    "            # hidden_states_chunk.append(outputs.hidden_states.cpu())\n",
    "\n",
    "            # if index % 256 == 0 and index > 0:\n",
    "            #     # Store the chunk.\n",
    "            #     torch.save(hidden_states_chunk, f\"hidden_states_{chunk_id}.pt\")\n",
    "            #     hidden_states_chunk.clear()\n",
    "            #     chunk_id += 1\n",
    "\n",
    "            if index % 1024 == 0 and index > 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b010030-e880-4439-8bbe-28747f3186d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
