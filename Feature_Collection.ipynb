{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469678bf-d4fe-4f0e-88f8-a18c6bd8bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41801a32-8304-4e60-8365-101c680149bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fec9114709472a83c384a6f6373534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\", torch_dtype=torch.bfloat16\n",
    ").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\"\n",
    ")\n",
    "\n",
    "with open(\"wikitext103-v1-filtered.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d090e2f-e861-4935-b557-6bbc17aa6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_padding = False\n",
    "\n",
    "# if add_padding:\n",
    "#     # https://discuss.huggingface.co/t/llama2-pad-token-for-batched-inference/48020\n",
    "#     # add_special_tokens doesn't work for some reason\n",
    "#     # tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "#     tokenizer.pad_token = \"[PAD]\"\n",
    "#     tokenizer.padding_side = \"left\"\n",
    "    \n",
    "#     # Mini-test\n",
    "#     def mini_test():\n",
    "#         tokenization = tokenizer(texts[:1], padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "#         sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "#         print(sequence_lengths)\n",
    "#         # print(tokenization.input_ids[0, -sequence_lengths[0]:])\n",
    "#         # print(tokenizer.decode(tokenization.input_ids[0]))\n",
    "#         # assert (tokenization.input_ids[0, -sequence_lengths[0]:] > 0).all()\n",
    "    \n",
    "#     mini_test()\n",
    "\n",
    "# shuffled_index = 0\n",
    "# tokenization = tokenizer(\n",
    "#     texts[shuffled_index],\n",
    "#     return_tensors='pt',\n",
    "#     return_attention_mask=True,\n",
    "#     padding=False,\n",
    "# ).to('cuda')\n",
    "# sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.forward(\n",
    "#         **tokenization,\n",
    "#         output_hidden_states=True,\n",
    "#     )\n",
    "#     # (1 + n_layers) tuple of tensors [batch_size, sequence_length, d_model=4096]\n",
    "#     hidden_states = outputs.hidden_states\n",
    "\n",
    "# hidden_states[-1].view(-1, 4096)[0].abs().sum()\n",
    "# (encoder.encode.weight @ hidden_states[-1].view(-1, 4096)[[1], :].T)\n",
    "# hidden_states[-1].view(-1, 4096).T\n",
    "\n",
    "# torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "# torch.save(encoder_optim.state_dict(), \"encoder_optim.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "474c0a26-5cf5-4510-98af-a2821de00af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "import quantized_autoencoder\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cfg = {\n",
    "    \"llm_dims\": 4096,\n",
    "    \"sparse_dims\": 4096 * 64,\n",
    "    \"topk\": 4096,\n",
    "    \"enc_dtype\": \"bf16\",\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "# at some point: create a dictionary of models / optimizers\n",
    "# might eventually have one model per layer\n",
    "encoder = quantized_autoencoder.QSAE(cfg=cfg).to(device='cuda')\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d32691ce-5e24-491f-b02f-b0b84570824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ddi7csxp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▂▁▅▁▂▁▁▁▃▃▅▃▆▁▁▂▁▁▄▁▂█▁▂▁▂▂▂▃▁▂▁▃▃▂▁▁▂▁</td></tr><tr><td>quantization_loss</td><td>▄▄▅▅▁▁▄▃▄▆▅▃▄█▃▂▄▁▅▂▄▆▃▂▄▁▂▃▃▄▂▂▄▄▂▆▂▄▁▃</td></tr><tr><td>reconstruction_loss</td><td>▂▂▁▅▁▂▁▁▁▃▃▅▃▆▁▁▂▁▁▄▁▂█▁▂▁▂▂▂▃▁▂▁▃▃▂▁▁▂▁</td></tr><tr><td>std_dev</td><td>▂▃▂▆▂▃▁▂▁▅▄▆▄▇▁▁▃▁▂▅▁▃█▂▂▁▂▃▃▄▂▃▂▄▄▄▁▁▃▂</td></tr><tr><td>unquantized_total</td><td>▃▂▃▂▄▅▅▃▄▁▂▁▃▁▅▄▂▅▃▂▄▂▁▄▃█▃▄▃▂▄▂▄▂▃▂▄▆▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>3.85006</td></tr><tr><td>quantization_loss</td><td>0.62829</td></tr><tr><td>reconstruction_loss</td><td>3.22177</td></tr><tr><td>std_dev</td><td>1.6875</td></tr><tr><td>unquantized_total</td><td>17218721.68421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-smoke-35</strong> at: <a href='https://wandb.ai/myfatemi/llm-mechanics/runs/ddi7csxp' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics/runs/ddi7csxp</a><br/> View job at <a href='https://wandb.ai/myfatemi/llm-mechanics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzcwNDk4NQ==/version_details/v7' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzcwNDk4NQ==/version_details/v7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_201948-ddi7csxp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ddi7csxp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9955fc9ff416eaf681698228c069f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112046766922706, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/weka/scratch/gsk6me/NLP_Project/wandb/run-20240311_202323-qu5ik63v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/myfatemi/llm-mechanics/runs/qu5ik63v' target=\"_blank\">effortless-pyramid-36</a></strong> to <a href='https://wandb.ai/myfatemi/llm-mechanics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/myfatemi/llm-mechanics' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/myfatemi/llm-mechanics/runs/qu5ik63v' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics/runs/qu5ik63v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   1%|          | 3942/749962 [06:47<21:26:25,  9.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m             index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    118\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 120\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# batch = texts[index:index + batch_size]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     shuffled_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(order[index])\n\u001b[1;32m     20\u001b[0m     tokenization \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshuffled_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     sequence_lengths \u001b[38;5;241m=\u001b[39m tokenization\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    # wandb.init(mode='dryrun')\n",
    "    wandb.init(project='llm-mechanics', config={'sparse_dims': 4096 * 128, 'topk': 4096, 'layer': 16})\n",
    "\n",
    "    shuffle = True\n",
    "    if shuffle:\n",
    "        torch.manual_seed(0)\n",
    "        order = torch.randperm(len(texts), device='cpu')\n",
    "    else:\n",
    "        order = torch.arange(len(texts), device='cpu')\n",
    "    \n",
    "    index = 0\n",
    "    # batch_size = 1\n",
    "    chunk_id = 0\n",
    "    counter = 0\n",
    "    with tqdm.tqdm(desc='Running inference', total=len(texts)) as pbar:\n",
    "        while index < len(texts):\n",
    "            # batch = texts[index:index + batch_size]\n",
    "            shuffled_index = int(order[index])\n",
    "            tokenization = tokenizer(\n",
    "                texts[shuffled_index],\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=True,\n",
    "                padding=False,\n",
    "            ).to('cuda')\n",
    "            sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(\n",
    "                    **tokenization,\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                # (1 + n_layers) tuple of tensors [batch_size, sequence_length, d_model=4096]\n",
    "                hidden_states = outputs.hidden_states\n",
    "\n",
    "            # calculate reconstruction loss for autoencoder and train weights\n",
    "            # acts = torch.stack(hidden_states, dim=0).view(-1, 4096)\n",
    "\n",
    "            # constrain to only the nth layer\n",
    "            # hidden_states[0] corresponds to embeddings.\n",
    "            # hidden_states[16] is after the 16th transformer layer.\n",
    "            acts = hidden_states[16].view(-1, 4096)\n",
    "            \n",
    "            # calculate gradient in batches to enable higher sparsity\n",
    "            use_qsae = True\n",
    "            if use_qsae:\n",
    "                act_batch_i = 0\n",
    "                act_bs = 1024\n",
    "                q_tot = 0\n",
    "                r_tot = 0\n",
    "                unquantized_tot = 0\n",
    "                \n",
    "                encoder_optim.zero_grad()\n",
    "                \n",
    "                std_dev = torch.std(acts).item()\n",
    "                # mean = torch.mean(acts, dim=0)\n",
    "                while act_batch_i < len(acts):\n",
    "                    act_batch = acts[act_batch_i:act_batch_i + act_bs]\n",
    "    \n",
    "                    _x_reconstructed, _scores, _quantized_activations, quantization_error, reconstruction_error = encoder(act_batch)\n",
    "\n",
    "                    loss = quantization_error + reconstruction_error\n",
    "                    \n",
    "                    loss_scaled = (loss * len(act_batch) / len(acts))\n",
    "                    loss_scaled.backward()\n",
    "    \n",
    "                    q_tot += (quantization_error * len(act_batch)).item()\n",
    "                    r_tot += (reconstruction_error * len(act_batch)).item()\n",
    "                    unquantized_tot += (_scores * len(act_batch)).sum().item()\n",
    "                    \n",
    "                    act_batch_i += act_bs\n",
    "                \n",
    "                encoder_optim.step()\n",
    "                \n",
    "                wandb.log({\n",
    "                    'reconstruction_loss': r_tot/len(acts),\n",
    "                    'quantization_loss': q_tot/len(acts),\n",
    "                    'unquantized_total': unquantized_tot/len(acts),\n",
    "                    'loss': (r_tot + q_tot)/len(acts),\n",
    "                    'std_dev': std_dev,\n",
    "                    # 'mean': mean.mean(),\n",
    "                })\n",
    "            else:\n",
    "                act_batch_i = 0\n",
    "                act_bs = 1024\n",
    "                l1_tot = 0\n",
    "                l2_tot = 0\n",
    "                while act_batch_i < len(acts):\n",
    "                    act_batch = acts[act_batch_i:act_batch_i + act_bs]\n",
    "    \n",
    "                    loss, x_reconstruct, mid_acts, l2_loss, l1_loss = encoder(act_batch)\n",
    "                    loss_scaled = (loss * len(act_batch) / len(acts))\n",
    "                    loss_scaled.backward()\n",
    "    \n",
    "                    l1_tot += (l1_loss * len(act_batch)).item()\n",
    "                    l2_tot += (l2_loss * len(act_batch)).item()\n",
    "                    act_batch_i += act_bs\n",
    "                \n",
    "                encoder.make_decoder_weights_and_grad_unit_norm()\n",
    "                encoder_optim.step()\n",
    "                encoder_optim.zero_grad()\n",
    "            \n",
    "                wandb.log({'l2_loss': l2_tot/len(acts), 'l1_loss': l1_tot/len(acts), 'loss': (l1_tot+l2_tot)/len(acts)})\n",
    "\n",
    "            # if index % 256 == 0 and index > 0:\n",
    "            #     # Store the chunk.\n",
    "            #     torch.save(hidden_states_chunk, f\"hidden_states_{chunk_id}.pt\")\n",
    "            #     hidden_states_chunk.clear()\n",
    "            #     chunk_id += 1\n",
    "\n",
    "            if index % 1024 == 0 and index > 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            if index % 4096 == 0 and index > 0:\n",
    "                torch.save(encoder.state_dict(), \"encoder.pt\")\n",
    "                torch.save(encoder_optim.state_dict(), \"encoder_optim.pt\")\n",
    "\n",
    "            index += 1\n",
    "            pbar.update()\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f3db2-561b-48f2-8cbc-14fb5b882414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
