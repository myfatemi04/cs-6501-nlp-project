{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59554e51-d6eb-4218-bcfa-2f7bd2d835b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469678bf-d4fe-4f0e-88f8-a18c6bd8bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41801a32-8304-4e60-8365-101c680149bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cbc74dd98646e08a0dd6be12e1d034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\", torch_dtype=torch.bfloat16\n",
    ").to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"./llama-huggingface/llama-2-7b-chat\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85929d39-5f56-45f2-8f87-b542434ec837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"wikitext103-v1-filtered.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6a1286-6ee1-4b04-b891-0006673ae693",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_padding = False\n",
    "\n",
    "if add_padding:\n",
    "    # https://discuss.huggingface.co/t/llama2-pad-token-for-batched-inference/48020\n",
    "    # add_special_tokens doesn't work for some reason\n",
    "    # tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    tokenizer.pad_token = \"[PAD]\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Mini-test\n",
    "    def mini_test():\n",
    "        tokenization = tokenizer(texts[:1], padding=True, return_attention_mask=True, return_tensors='pt')\n",
    "        sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "        print(sequence_lengths)\n",
    "        # print(tokenization.input_ids[0, -sequence_lengths[0]:])\n",
    "        # print(tokenizer.decode(tokenization.input_ids[0]))\n",
    "        # assert (tokenization.input_ids[0, -sequence_lengths[0]:] > 0).all()\n",
    "    \n",
    "    mini_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e315ad3f-dc26-438a-9527-145764cf0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of models / optimizers\n",
    "\n",
    "import autoencoder\n",
    "\n",
    "cfg = {\n",
    "    \"act_size\": 4096,\n",
    "    \"dict_size\": 4096 * 128,\n",
    "    \"enc_dtype\": \"bf16\",\n",
    "    \"l1_coeff\": 3e-3,\n",
    "    \"seed\": 0,\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "# might eventually have one model per layer\n",
    "encoder = autoencoder.AutoEncoder(cfg=cfg)\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d32691ce-5e24-491f-b02f-b0b84570824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyfatemi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/weka/scratch/gsk6me/NLP_Project/wandb/run-20240311_101246-rj1v45i5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/myfatemi/llm-mechanics/runs/rj1v45i5' target=\"_blank\">smart-wave-1</a></strong> to <a href='https://wandb.ai/myfatemi/llm-mechanics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/myfatemi/llm-mechanics' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/myfatemi/llm-mechanics/runs/rj1v45i5' target=\"_blank\">https://wandb.ai/myfatemi/llm-mechanics/runs/rj1v45i5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference:   1%|          | 5866/749962 [12:56<27:22:04,  7.55it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m             index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     49\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m loss, x_reconstruct, mid_acts, l2_loss, l1_loss \u001b[38;5;241m=\u001b[39m encoder(acts)\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43ml2_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: l1_loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n\u001b[1;32m     35\u001b[0m encoder\u001b[38;5;241m.\u001b[39mmake_decoder_weights_and_grad_unit_norm()\n\u001b[1;32m     36\u001b[0m encoder_optim\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "def run():\n",
    "    wandb.init(project='llm-mechanics', config={'expansion_factor': 128})\n",
    "    \n",
    "    index = 0\n",
    "    # batch_size = 1\n",
    "    chunk_id = 0\n",
    "    counter = 0\n",
    "    with tqdm.tqdm(desc='Running inference', total=len(texts)) as pbar:\n",
    "        while index < len(texts):\n",
    "            # batch = texts[index:index + batch_size]\n",
    "            tokenization = tokenizer(\n",
    "                texts[index],\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=True,\n",
    "                padding=False,\n",
    "            ).to('cuda')\n",
    "            sequence_lengths = tokenization.attention_mask.sum(dim=-1)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.forward(\n",
    "                    **tokenization,\n",
    "                    output_hidden_states=True,\n",
    "                )\n",
    "                # (1 + n_layers) tuple of tensors [batch_size, sequence_length, d_model=4096]\n",
    "                hidden_states = outputs.hidden_states\n",
    "\n",
    "            # calculate reconstruction loss for autoencoder and train weights\n",
    "            acts = torch.stack(hidden_states, dim=0).view(-1, 4096)\n",
    "            loss, x_reconstruct, mid_acts, l2_loss, l1_loss = encoder(acts)\n",
    "            loss.backward()\n",
    "            wandb.log({'l2_loss': l2_loss.item(), 'l1_loss': l1_loss.item(), 'loss': loss.item()})\n",
    "            encoder.make_decoder_weights_and_grad_unit_norm()\n",
    "            encoder_optim.step()\n",
    "            encoder_optim.zero_grad()\n",
    "\n",
    "            # if index % 256 == 0 and index > 0:\n",
    "            #     # Store the chunk.\n",
    "            #     torch.save(hidden_states_chunk, f\"hidden_states_{chunk_id}.pt\")\n",
    "            #     hidden_states_chunk.clear()\n",
    "            #     chunk_id += 1\n",
    "\n",
    "            if index % 1024 == 0 and index > 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            index += 1\n",
    "            pbar.update()\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b010030-e880-4439-8bbe-28747f3186d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
