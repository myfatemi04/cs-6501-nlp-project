{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b81b403-e2ff-4404-a868-80760497777e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dec6c7-10fe-40f3-8c2f-4298aca98bd3",
   "metadata": {},
   "source": [
    "## Load `Llama 2` Model\n",
    "\n",
    "This model is hosted on [HuggingFace](https://huggingface.co/microsoft/phi-2).\n",
    "\n",
    "Requirements:\n",
    " * `transformers`\n",
    " * `datasets`\n",
    " * `torch`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a2c91f-e77d-47ee-b518-6cf6159b99d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a09847ef-2eb2-4bd6-91df-8e3cfb771b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3030d915d884cacac3101472f5b339c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./llama-huggingface/llama-2-7b-chat\", torch_dtype=\"auto\") # , attn_implementation=\"flash_attention_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama-huggingface/llama-2-7b-chat\") # , attn_implementation=\"flash_attention_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ca5eb-9477-4dcd-bef5-935eaf145e54",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll use a small and quick dataset for testing: Winogrande. This is stored on [HuggingFace](https://huggingface.co/datasets/winogrande) as well. This returns a `DatasetDict` with the keys `train`, `test`, and `validation`. This can be indexed just like a Torch dataset. For Winogrande, each item contains the following indices:\n",
    " * `sentence`: `\"A sentence with an _\"`\n",
    " * `option{1, 2}`: Possible ways to fill in the blank\n",
    " * `answer`: `1` or `2` indicating the correct option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcdb4591-077a-473a-8e1e-b4fa2d249255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"User: What is the University of Virginia? Please be as descriptive as possible.\\nAssistant:\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200, output_hidden_states=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39cfefbc-3b16-4212-8194-f9da6bf21fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(model.forward(**inputs, output_hidden_states=True).hidden_states[0][0])\n",
    ")\n",
    "print(inputs['input_ids'].shape)\n",
    "# (batch_size, sequence_length, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394d4d0e-a71e-4dad-bc4d-9d86f408c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Alice: What is the University of Virginia? Please be as descriptive as possible.\n",
      "Bob: The University of Virginia is a public research university located in Charlottesville, Virginia. It was founded in 1819 by Thomas Jefferson, who designed the campus to be a symbol of the principles of liberty and education. The university is known for its strong programs in engineering, business, law, medicine, and the arts and sciences. It has a diverse student body of around 20,000 students and offers a wide range of undergraduate and graduate degree programs. The university is also home to several research centers and institutes, including the Miller Center for Public Affairs and the Institute for Advanced Studies in Culture.\n",
      "Alice: Wow, that's really interesting! Can you tell me more about the history of the university?\n",
      "Bob: Of course! The University of Virginia was founded by Thomas Jefferson, who was a prominent statesman and architect. Jefferson was a strong believer in the importance of education and saw the university as a\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.batch_decode(outputs['sequences'])[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "541f0fe5-8dd6-4aea-92dd-b55edafe6ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states[0][0])\n",
    "# for state in outputs.hidden_states[0]:\n",
    "#     print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7129d24-b0f4-4dd3-820e-4a79a4a2248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on Winogrande\n",
    "from datasets import load_dataset\n",
    "\n",
    "winogrande_dataset = load_dataset('winogrande', 'winogrande_l')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a36b9-5231-4e05-ae06-1740756fe56c",
   "metadata": {},
   "source": [
    "We'll create a quick prompt template and see how `phi-2` responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ea0a1d-5c0a-4f95-9e15-d6104f479160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/gsk6me/miniconda3/envs/rlbench/lib/python3.8/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 55, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill in the blank: \"She remembered how annoying it is to dust her wood chair so she bought a plastic table instead.  Cleaning the _ is time consuming.\"\n",
      "What of these options is more correct?\n",
      "A) chair\n",
      "B) table\n",
      "\n",
      "Answer: B\n",
      "True output: A\n"
     ]
    }
   ],
   "source": [
    "def create_winogrande_prompt(item):\n",
    "    return f\"\"\"\n",
    "Fill in the blank: \"{item['sentence']}\"\n",
    "What of these options is more correct?\n",
    "A) {item['option1']}\n",
    "B) {item['option2']}\n",
    "\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "item = winogrande_dataset['train'][15]\n",
    "\n",
    "inputs = tokenizer(create_winogrande_prompt(item), return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=1)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(text)\n",
    "print(\"True output:\", ' AB'[int(item['answer'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f981e-06af-4c15-ac79-eb980d3a3382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLBench",
   "language": "python",
   "name": "rlbench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
