{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import PreTrainedModel\n",
    "import seaborn as sns\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ad800ec8754512be993f420e5997d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "CACHE_DIR = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "MODEL_NAME = \"microsoft/phi-2\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "HF_TOKEN = \"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\"\n",
    "TORCH_DTYPE = torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=TORCH_DTYPE, device_map=\"cuda\", cache_dir=CACHE_DIR, token=HF_TOKEN)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR, token=HF_TOKEN)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        #layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear =nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    \n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        #Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        #Adding scaled unit gaussian noise to the logits\n",
    "        noise = torch.randn_like(logits)*F.softplus(noise_logits)\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float('-inf'))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices\n",
    "\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, mlp, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([copy.deepcopy(mlp).to(DEVICE) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_embed:  2560\n",
      "Existing MLP module: <class 'transformers.models.phi.modeling_phi.PhiMLP'>\n",
      "SparseMoE(\n",
      "  (router): NoisyTopkRouter(\n",
      "    (topkroute_linear): Linear(in_features=2560, out_features=4, bias=True)\n",
      "    (noise_linear): Linear(in_features=2560, out_features=4, bias=True)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0-3): 4 x PhiMLP(\n",
      "      (activation_fn): NewGELUActivation()\n",
      "      (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "      (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiAttention(\n",
      "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): SparseMoE(\n",
      "          (router): NoisyTopkRouter(\n",
      "            (topkroute_linear): Linear(in_features=2560, out_features=4, bias=True)\n",
      "            (noise_linear): Linear(in_features=2560, out_features=4, bias=True)\n",
      "          )\n",
      "          (experts): ModuleList(\n",
      "            (0-3): 4 x PhiMLP(\n",
      "              (activation_fn): NewGELUActivation()\n",
      "              (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "              (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def create_moe(model: PreTrainedModel, num_experts=4, top_k=2):\n",
    "    # check if model has already been converted to MoE\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, SparseMoE):\n",
    "            print(\"Model is already converted to MoE.\")\n",
    "            return model\n",
    "\n",
    "    # 1. get embedding size\n",
    "    attention = model.model.layers[0].self_attn\n",
    "    n_embed = attention.q_proj.in_features\n",
    "    print(\"n_embed: \", n_embed)\n",
    "\n",
    "    # 2. Find the existing MLP module\n",
    "    existing_mlp = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == \"model.layers.1.mlp\":\n",
    "            existing_mlp = module\n",
    "            break\n",
    "\n",
    "    if existing_mlp is None:\n",
    "        raise ValueError(\"Could not find the MLP module in the model.\")\n",
    "\n",
    "    print(\"Existing MLP module:\", type(existing_mlp))\n",
    "\n",
    "    # 3. Build the SparseMoE model\n",
    "    moe = SparseMoE(existing_mlp, n_embed, num_experts, top_k)\n",
    "    moe.to(DEVICE)\n",
    "    print(moe)\n",
    "\n",
    "    # 4. Replace the existing MLP module with the SparseMoE model\n",
    "    # Collect the names and modules to be replaced\n",
    "    modules_to_replace = {}\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, type(existing_mlp)):\n",
    "            modules_to_replace[name] = module\n",
    "\n",
    "    # Replace the modules\n",
    "    for name, module in modules_to_replace.items():\n",
    "        # Find the parent module and replace the original module with the new one\n",
    "        if '.' in name:\n",
    "            parent_name, child_name = name.rsplit('.', 1)\n",
    "            parent_module = dict(model.named_modules())[parent_name]\n",
    "            setattr(parent_module, child_name, moe)\n",
    "        else:\n",
    "            # For top-level modules\n",
    "            setattr(model, name, moe)\n",
    "\n",
    "    return model\n",
    "\n",
    "moe_model = create_moe(model)\n",
    "print(moe_model.to(DEVICE))\n",
    "assert next(moe_model.parameters()).is_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 4299,  3601,    62, 35505,     7,    77,  2599,   198, 50285, 37811,\n",
      "           198, 50285, 18557,   477,   778,   999,  1022,   352,   290,   299,\n",
      "           198, 50285, 37811]], device='cuda:0')}\n",
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"ibusultnar\n",
      "pricallys the V agprlordsedingingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsingsings\n"
     ]
    }
   ],
   "source": [
    "# outputs = model.generate(**inputs, max_length=200)\n",
    "print(inputs)\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
