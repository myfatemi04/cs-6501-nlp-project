{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "CACHE_DIR = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "# Optional; can help when memory is tight.\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HellaSwag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog',\n",
       " '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy.',\n",
       " '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits.',\n",
       " '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments.',\n",
       " 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Rowan/hellaswag\", \"en-US\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "selected_indices = np.random.choice(len(dataset[\"ctx\"]), 5, replace=False)\n",
    "selected_prompts = [dataset[\"ctx\"][i] for i in selected_indices]\n",
    "\n",
    "selected_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f414cdc0ee04e1c80c19fc1e1ec7725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def generate(model, tokenizer, prompt, do_display=True, max_new_tokens=50):\n",
    "    input_string = prompt\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\", return_attention_mask=True).to('cuda')\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=0)\n",
    "    text = tokenizer.batch_decode(outputs)[0][len(input_string):]\n",
    "\n",
    "    if do_display:\n",
    "        display(HTML(f\"<pre>{input_string}</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'>{text}<pre>\"))\n",
    "\n",
    "    return inputs['input_ids'][0], outputs[0], text\n",
    "\n",
    "def run_coding_sample(model, tokenizer, display=True):\n",
    "    prompt = '''def print_prime(n):\n",
    "       \"\"\"\n",
    "       Print all primes between 1 and n\n",
    "       \"\"\"'''\n",
    "    return generate(model, tokenizer, prompt, display)\n",
    "\n",
    "def run_coding_sample_2(model, tokenizer):\n",
    "    prompt = '''def array_sum(array, nrows, ncols):\n",
    "       \"\"\"\n",
    "       Use two for loops to add elements of an array.\n",
    "       \"\"\"'''\n",
    "    return generate(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 29 to 26"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> George Washington.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 29 to 29"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> George Washington.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hooked_phi import attach_hooks, detach_hooks\n",
    "\n",
    "all_results = []\n",
    "all_results_tokenized = []\n",
    "\n",
    "# Returns a hook that can be used to ablate a set of neurons.\n",
    "def ablate_neurons(mask):\n",
    "    assert mask.shape[0] == model.config.num_hidden_layers\n",
    "    assert mask.shape[1] == model.config.intermediate_size\n",
    "\n",
    "    def hook(neurons, layer_idx):\n",
    "        neurons[..., ~mask[layer_idx]] = 0\n",
    "        return neurons\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Ablate the last layer MLP.\n",
    "for (start_layer, end_layer) in [(25, 26), (28, 29)]:\n",
    "    if num_ablation_layers == 0:\n",
    "        display(Markdown(f'## Baseline'))\n",
    "    else:\n",
    "        display(Markdown(f'## Ablating MLPs {32 - num_ablation_layers + 1} to {end_layer}'))\n",
    "    \n",
    "    mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    if num_ablation_layers > 0:\n",
    "        mask[start_layer:end_layer, :] = False\n",
    "    \n",
    "    attach_hooks(model.model, ablate_neurons(mask))\n",
    "    \n",
    "    # input_ids, input_and_completion_ids, result = run_coding_sample(model, tokenizer)\n",
    "    input_ids, input_and_completion_ids, result = generate(model, tokenizer, \"Instruct: Who was the first United States president?\\n\\nOutput: Their name was\", do_display=True)\n",
    "    if num_ablation_layers == 0:\n",
    "        ground_truth = result\n",
    "    \n",
    "    all_results.append(result)\n",
    "\n",
    "detach_hooks(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluator import perplexity_evaluator\n",
    "\n",
    "perplexity_result = perplexity_evaluator(preds=all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog is running with a ball in its mouth. the dog is running towards a person. the person is smiling and waving at the dog. the text says \"Meet Max, the best dog in the world. He loves to play fetch and make new friends', '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy. [substeps] Cognitive behavioral therapy (CBT) is a common approach that helps people identify and change negative thought patterns. Exposure therapy is another approach that involves gradually exposing people to their fears in a safe and controlled environment. [substeps] Med', '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits. Instead, try to remain calm and avoid reacting defensively. [substeps]\\n#*For example, if someone says, “I’m not mad at you,” you might respond, “I’m not mad', '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments. [step] The higher the nrr, the more effective the hearing protection device is. [step] The nrr is measured in decibels (db) and is expressed as a number. For example, a hearing protection device with an n', 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he is wearing a white shirt and black pants. He is holding a tool in his hand.\\n\\nQuestion 2:\\nThe man is using a tool to hit something on the table before sanding it. What is the tool?\\n\\nAnswer 2']\n"
     ]
    }
   ],
   "source": [
    "from evaluator import charcut_evaluator\n",
    "\n",
    "ground_truths = []\n",
    "predictions = {}\n",
    "\n",
    "for prompt in selected_prompts:\n",
    "    \n",
    "    # Ablate the last layer MLP.\n",
    "    for num_ablation_layers in range(0, 5, 2):\n",
    "        \n",
    "        mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "        if num_ablation_layers > 0:\n",
    "            mask[-num_ablation_layers:, :] = False\n",
    "        \n",
    "        attach_hooks(model.model, ablate_neurons(mask))\n",
    "        \n",
    "        input_ids, input_and_completion_ids, result = generate(model, tokenizer, prompt, do_display=False)\n",
    "        \n",
    "        if num_ablation_layers == 0:\n",
    "            ground_truths.append(prompt+result)\n",
    "        \n",
    "        if num_ablation_layers not in predictions:\n",
    "            predictions[num_ablation_layers] = []\n",
    "        \n",
    "        predictions[num_ablation_layers].append(prompt+result)\n",
    "        \n",
    "\n",
    "    detach_hooks(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog is running with a ball in its mouth. the dog is running towards a person. the person is smiling and waving at the dog. the text says \"Meet Max, the best dog in the world. He loves to play fetch and make new friends', '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy. [substeps] Cognitive behavioral therapy (CBT) is a common approach that helps people identify and change negative thought patterns. Exposure therapy is another approach that involves gradually exposing people to their fears in a safe and controlled environment. [substeps] Med', '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits. Instead, try to remain calm and avoid reacting defensively. [substeps]\\n#*For example, if someone says, “I’m not mad at you,” you might respond, “I’m not mad', '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments. [step] The higher the nrr, the more effective the hearing protection device is. [step] The nrr is measured in decibels (db) and is expressed as a number. For example, a hearing protection device with an n', 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he is wearing a white shirt and black pants. He is holding a tool in his hand.\\n\\nQuestion 2:\\nThe man is using a tool to hit something on the table before sanding it. What is the tool?\\n\\nAnswer 2']\n"
     ]
    }
   ],
   "source": [
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for key, value in predictions.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charcut_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Visualizing How Log-Likelihood Evolves, and For Which Tokens"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Baseline"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: 0.0\n",
      "Worst deflection: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;40</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>.</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 23"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.14229658246040344\n",
      "Worst deflection: -0.41537782549858093\n",
      "Expected worst deflection (SHAP): -0.5570576071739197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 140.30931091308594, 140.30931091308594)'>&nbsp;40</span><span class='token' style='background-color:rgb(240, 233.65481567382812, 233.65481567382812)'>.</span><span class='token' style='background-color:rgb(236.4176788330078, 240, 236.4176788330078)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 24"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.08294917643070221\n",
      "Worst deflection: -0.11641338467597961\n",
      "Expected worst deflection (SHAP): -0.16418358257838658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 212.060791015625, 212.060791015625)'>&nbsp;40</span><span class='token' style='background-color:rgb(240, 228.19796752929688, 228.19796752929688)'>.</span><span class='token' style='background-color:rgb(240, 220.01783752441406, 220.01783752441406)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 25"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.06327449530363083\n",
      "Worst deflection: -0.10350129008293152\n",
      "Expected worst deflection (SHAP): -0.11770982543627422\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 215.15968322753906, 215.15968322753906)'>&nbsp;40</span><span class='token' style='background-color:rgb(240, 235.9504852294922, 235.9504852294922)'>.</span><span class='token' style='background-color:rgb(240, 223.3321990966797, 223.3321990966797)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 26"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.09004755318164825\n",
      "Worst deflection: -0.3358364999294281\n",
      "Expected worst deflection (SHAP): -0.38859007093641496\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 159.39923095703125, 159.39923095703125)'>&nbsp;40</span><span class='token' style='background-color:rgb(231.56832885742188, 240, 231.56832885742188)'>.</span><span class='token' style='background-color:rgb(232.66513061523438, 240, 232.66513061523438)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 27"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.06138762831687927\n",
      "Worst deflection: -0.22044476866722107\n",
      "Expected worst deflection (SHAP): -0.2223598137497902\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 187.09324645996094, 187.09324645996094)'>&nbsp;40</span><span class='token' style='background-color:rgb(238.87611389160156, 240, 238.87611389160156)'>.</span><span class='token' style='background-color:rgb(232.41622924804688, 240, 232.41622924804688)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 28"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.035411231219768524\n",
      "Worst deflection: -0.09275761246681213\n",
      "Expected worst deflection (SHAP): -0.1680041319794125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 217.73817443847656, 217.73817443847656)'>&nbsp;40</span><span class='token' style='background-color:rgb(234.54029846191406, 240, 234.54029846191406)'>.</span><span class='token' style='background-color:rgb(240, 231.3060302734375, 231.3060302734375)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 29"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.18967251479625702\n",
      "Worst deflection: -0.2672095000743866\n",
      "Expected worst deflection (SHAP): -0.2871393635869026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 175.86972045898438, 175.86972045898438)'>&nbsp;40</span><span class='token' style='background-color:rgb(240, 195.48446655273438, 195.48446655273438)'>.</span><span class='token' style='background-color:rgb(240, 212.0815887451172, 212.0815887451172)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 30"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.09360592812299728\n",
      "Worst deflection: -0.2958889901638031\n",
      "Expected worst deflection (SHAP): -0.25434938818216324\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(240, 168.98663330078125, 168.98663330078125)'>&nbsp;40</span><span class='token' style='background-color:rgb(217.55162048339844, 240, 217.55162048339844)'>.</span><span class='token' style='background-color:rgb(240, 221.16871643066406, 221.16871643066406)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLP 31"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.23373013734817505\n",
      "Worst deflection: -0.850031852722168\n",
      "Expected worst deflection (SHAP): -0.8379189703199599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>5</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;times</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;8</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;equals</span><span class='token' style='background-color:rgb(171.90896606445312, 240, 171.90896606445312)'>&nbsp;40</span><span class='token' style='background-color:rgb(240, 207.63092041015625, 207.63092041015625)'>.</span><span class='token' style='background-color:rgb(240, 35.99235534667969, 35.99235534667969)'>\n",
       "</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is called \"neuron vis\", but can really be used for anything involving a score assigned to each token.\n",
    "# In this case, visualize logprob\n",
    "from neuron_visualization import basic_neuron_vis, basic_neuron_vis_signed\n",
    "\n",
    "# Get granular nll estimates\n",
    "display(Markdown(f'# Visualizing How Log-Likelihood Evolves, and For Which Tokens'))\n",
    "\n",
    "# 1. Get ground truth.\n",
    "detach_hooks(model.model)\n",
    "input_ids, input_and_completion_ids, ground_truth = generate(model, tokenizer, \"5 times 8 equals\", do_display=False, max_new_tokens=3)\n",
    "completion_ids = input_and_completion_ids[len(input_ids):]\n",
    "\n",
    "# Calculate baseline logprobs.\n",
    "predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "logits = predictions_for_next_token[0][0]\n",
    "logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "baseline_logprobs_for_sampled_output_tokens = logprobs_for_output_tokens[\n",
    "    torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "    completion_ids\n",
    "]\n",
    "\n",
    "# 2. Ablate model. See which suddens suddenly become highly unlikely (by visualizing negative logprobs).\n",
    "# for ablation_layer in range(31, 31 - 8 - 1, -1):\n",
    "# # for ablation_layer in range(31, -1, -1):\n",
    "#     mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    \n",
    "#     display(Markdown(f'## Ablating MLP {ablation_layer + 1} alone'))\n",
    "#     mask[ablation_layer, :] = False\n",
    "\n",
    "# for num_ablation_layers in range(0, 5, 1):\n",
    "#     mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "#     if num_ablation_layers == 0:\n",
    "#         display(Markdown(f'## Baseline'))\n",
    "#     else:\n",
    "#         display(Markdown(f'## Ablating MLPs {32 - num_ablation_layers + 1} to 32'))\n",
    "for ablate_layer in [-1, 23, 24, 25, 26, 27, 28, 29, 30, 31]:\n",
    "    # if num_ablation_layers > 0:\n",
    "    #     mask[-num_ablation_layers:, :] = False\n",
    "    if ablate_layer == -1:\n",
    "        display(Markdown(f'## Baseline'))\n",
    "    else:\n",
    "        display(Markdown(f'## Ablating MLP {ablate_layer}'))\n",
    "\n",
    "    mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)    \n",
    "    if ablate_layer > 0:\n",
    "        mask[ablate_layer, :] = False\n",
    "\n",
    "    attach_hooks(model.model, ablate_neurons(mask))\n",
    "    predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "    logits = predictions_for_next_token[0][0]\n",
    "    logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "    logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "    logprobs_for_sampled_output_tokens = logprobs_for_output_tokens[\n",
    "        torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "        completion_ids\n",
    "    ]\n",
    "\n",
    "    # visualized_tokens = outputs[0].cpu()[len(inputs['input_ids'][0].cpu()):]\n",
    "    visualized_tokens = input_and_completion_ids.cpu()\n",
    "    token_names = [\n",
    "        tokenizer.decode(torch.tensor([visualized_tokens[i]]))\n",
    "        for i in range(len(visualized_tokens))\n",
    "    ]\n",
    "\n",
    "    deflections = logprobs_for_sampled_output_tokens - baseline_logprobs_for_sampled_output_tokens\n",
    "\n",
    "    colors = torch.cat([torch.zeros_like(input_ids), deflections])\n",
    "\n",
    "    print(\"Average deflection:\", deflections.mean().item())\n",
    "    print(\"Worst deflection:\", deflections.min().item())\n",
    "    if ablate_layer != -1:\n",
    "        print(\"Expected worst deflection (SHAP):\", average_perturbations[ablate_layer])\n",
    "    html = basic_neuron_vis_signed(token_names, colors, 1)\n",
    "    display(HTML(html))\n",
    "\n",
    "detach_hooks(model.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Calculating Monte Carlo SHAP Scores Per Layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 times 8 equals 40.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Monte Carlo sampling: 100%|██████████| 128/128 [00:11<00:00, 11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -5.629\n",
      "Layer 1: 0.070\n",
      "Layer 2: 0.027\n",
      "Layer 3: -0.722\n",
      "Layer 4: -0.313\n",
      "Layer 5: -0.050\n",
      "Layer 6: -0.065\n",
      "Layer 7: -0.098\n",
      "Layer 8: -0.225\n",
      "Layer 9: -0.108\n",
      "Layer 10: -0.090\n",
      "Layer 11: 0.012\n",
      "Layer 12: -0.134\n",
      "Layer 13: -0.069\n",
      "Layer 14: -0.081\n",
      "Layer 15: -0.280\n",
      "Layer 16: 0.016\n",
      "Layer 17: -0.009\n",
      "Layer 18: -0.121\n",
      "Layer 19: -0.062\n",
      "Layer 20: -0.128\n",
      "Layer 21: 0.005\n",
      "Layer 22: -0.292\n",
      "Layer 23: -0.557\n",
      "Layer 24: -0.164\n",
      "Layer 25: -0.118\n",
      "Layer 26: -0.389\n",
      "Layer 27: -0.222\n",
      "Layer 28: -0.168\n",
      "Layer 29: -0.287\n",
      "Layer 30: -0.254\n",
      "Layer 31: -0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This is called \"neuron vis\", but can really be used for anything involving a score assigned to each token.\n",
    "# In this case, visualize logprob\n",
    "from neuron_visualization import basic_neuron_vis, basic_neuron_vis_signed\n",
    "import tqdm\n",
    "\n",
    "# Get granular nll estimates\n",
    "display(Markdown(f'# Calculating Monte Carlo SHAP Scores Per Layer'))\n",
    "\n",
    "# 1. Get ground truth.\n",
    "detach_hooks(model.model)\n",
    "input_ids, input_and_completion_ids, ground_truth = generate(model, tokenizer, \"5 times 8 equals\", do_display=False, max_new_tokens=3)\n",
    "completion_ids = input_and_completion_ids[len(input_ids):]\n",
    "\n",
    "print(tokenizer.decode(input_and_completion_ids))\n",
    "\n",
    "# Calculate baseline logprobs.\n",
    "predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "logits = predictions_for_next_token[0][0]\n",
    "logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "baseline_logprobs_for_sampled_output_tokens = logprobs_for_output_tokens[\n",
    "    torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "    completion_ids\n",
    "]\n",
    "\n",
    "# 2. Ablate model. See which suddens suddenly become highly unlikely (by visualizing negative logprobs).\n",
    "logged_perturbations = {}\n",
    "\n",
    "num_coalitions = 128\n",
    "for sampling_iteration in tqdm.tqdm(range(num_coalitions), desc='Running Monte Carlo sampling'):\n",
    "    # Choose coalition.\n",
    "    # coalition_size = torch.randint(3, 5, ())\n",
    "    coalition_size = torch.randint(1, 3, ())\n",
    "    # coalition_size = 1\n",
    "    coalition = torch.randperm(32)[:coalition_size]\n",
    "\n",
    "    # For each layer in this coalition, simulate its removal.\n",
    "    mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    if num_ablation_layers > 0:\n",
    "        mask[coalition, :] = False\n",
    "\n",
    "    attach_hooks(model.model, ablate_neurons(mask))\n",
    "    predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "    logits = predictions_for_next_token[0][0]\n",
    "    logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "    logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "    logprobs_for_sampled_output_tokens_base = logprobs_for_output_tokens[\n",
    "        torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "        completion_ids\n",
    "    ]\n",
    "    detach_hooks(model.model)\n",
    "\n",
    "    removed_features = []\n",
    "    deflections = []\n",
    "    \n",
    "    for test_feature_index in range(len(coalition)):\n",
    "        mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "        new_coalition = torch.cat([coalition[:test_feature_index], coalition[test_feature_index + 1:]], dim=0)\n",
    "        if num_ablation_layers > 0:\n",
    "            mask[new_coalition, :] = False\n",
    "\n",
    "        attach_hooks(model.model, ablate_neurons(mask))\n",
    "        predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "        logits = predictions_for_next_token[0][0]\n",
    "        logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "        logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "        logprobs_for_sampled_output_tokens_probed = logprobs_for_output_tokens[\n",
    "            torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "            completion_ids\n",
    "        ]\n",
    "        detach_hooks(model.model)\n",
    "        removed_features.append(test_feature_index)\n",
    "        \n",
    "        # \"probed\" has one less feature. So, the marginal contribution of the feature is measured by subtracting\n",
    "        # base - probed.\n",
    "        deflections.append(logprobs_for_sampled_output_tokens_base - logprobs_for_sampled_output_tokens_probed)\n",
    "\n",
    "        # Log the perturbation\n",
    "        feature_id = int(coalition[test_feature_index])\n",
    "        if feature_id not in logged_perturbations:\n",
    "            logged_perturbations[feature_id] = []\n",
    "\n",
    "        perturbation = (logprobs_for_sampled_output_tokens_base - logprobs_for_sampled_output_tokens_probed).min()\n",
    "        logged_perturbations[feature_id].append(perturbation.item())\n",
    "\n",
    "# Calculate approximate SHAP scores based on the perturbations we've logged.\n",
    "average_perturbations = {\n",
    "    key: sum(values) / len(values)\n",
    "    for (key, values) in logged_perturbations.items()\n",
    "}\n",
    "\n",
    "sorted_keys = sorted(average_perturbations.keys())\n",
    "\n",
    "for key in sorted_keys:\n",
    "    print(f\"Layer {key}: {average_perturbations[key]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing \"kernel\" of MLP intermediate states\n",
    "\n",
    "If there MLP intermediate states represent recall coefficients - with the MLP encoder representing detection, and the MLP decoder representing triggering of the new memory - we would expect that most MLP intermediate states would be 0 (as only a sparse number of \"memories\" should be activated at any given time), or that MLP intermediate states would be correlated with meaningful concepts. Let's test this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_neurons(neurons, layer_idx):\n",
    "    if layer_idx == 0:\n",
    "        log.append([])\n",
    "\n",
    "    log[-1].append(neurons)\n",
    "    \n",
    "    return neurons\n",
    "\n",
    "attach_hooks(model.model, log_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "inputs, outputs, text = run_coding_sample_2(model, tokenizer)\n",
    "\n",
    "layer_id = 0\n",
    "\n",
    "all_mlp_activations = []\n",
    "for i in range(len(log)):\n",
    "    # log[forward pass index][layer index][batch index, token index in forward pass, mlp neuron index]\n",
    "    mlp_activations = log[i][layer_id][0, -1, :]\n",
    "    all_mlp_activations.append(mlp_activations)\n",
    "\n",
    "stacked = torch.stack(all_mlp_activations, dim=0)\n",
    "\n",
    "# Scale each MLP intermediate neuron by the maximum absolute value\n",
    "scale_by_dim = torch.max(stacked.abs(), dim=0, keepdims=True).values\n",
    "stacked_scaled = stacked / (scale_by_dim + 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"MLP activations: Last Layer (Unscaled)\")\n",
    "plt.hist(stacked.view(-1).cpu(), bins=25)\n",
    "plt.xlabel(\"MLP activation level\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"MLP activations: Last Layer (Scaled per dim.)\")\n",
    "plt.hist(stacked_scaled.view(-1).cpu(), bins=25)\n",
    "plt.xlabel(\"MLP activation level\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "It seems that MLP activations are very sparse. Therefore, it will hopefully be relatively simple to find meaningful MLP neurons to visualize.\n",
    "\n",
    "To automatically determine which MLP activations are most interesting, I will calculate the variance of the activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron_visualization import basic_neuron_vis\n",
    "\n",
    "variance = stacked.var(dim=0)\n",
    "highest_to_lowest_variance = variance.argsort(descending=True)\n",
    "visualized_tokens = outputs[0].cpu()[len(inputs['input_ids'][0].cpu()):]\n",
    "\n",
    "token_names = [\n",
    "    tokenizer.decode(torch.tensor([visualized_tokens[i]]))\n",
    "    for i in range(len(visualized_tokens))\n",
    "]\n",
    "\n",
    "visualized_neurons = highest_to_lowest_variance[:100]\n",
    "\n",
    "for neuron_i in range(len(visualized_neurons)):\n",
    "    neuron_id = visualized_neurons[neuron_i]\n",
    "    html = basic_neuron_vis(token_names, stacked[:, neuron_id], layer=layer_id, neuron_index=neuron_id)\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
