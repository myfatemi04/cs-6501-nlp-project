{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Set HuggingFace cache directory to scratch to save space.\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "CACHE_DIR = '/scratch/' + os.environ['USER'] + '/huggingface_cache'\n",
    "# Optional; can help when memory is tight.\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HellaSwag Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog',\n",
       " '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy.',\n",
       " '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits.',\n",
       " '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments.',\n",
       " 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Rowan/hellaswag\", \"en-US\", split=\"train\", cache_dir=CACHE_DIR)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "selected_indices = np.random.choice(len(dataset[\"ctx\"]), 5, replace=False)\n",
    "selected_prompts = [dataset[\"ctx\"][i] for i in selected_indices]\n",
    "\n",
    "selected_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f414cdc0ee04e1c80c19fc1e1ec7725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "def generate(model, tokenizer, prompt, do_display=True, max_new_tokens=50):\n",
    "    input_string = prompt\n",
    "    inputs = tokenizer(input_string, return_tensors=\"pt\", return_attention_mask=True).to('cuda')\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=0)\n",
    "    text = tokenizer.batch_decode(outputs)[0][len(input_string):]\n",
    "\n",
    "    if do_display:\n",
    "        display(HTML(f\"<pre>{input_string}</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'>{text}<pre>\"))\n",
    "\n",
    "    return inputs['input_ids'][0], outputs[0], text\n",
    "\n",
    "def run_coding_sample(model, tokenizer, display=True):\n",
    "    prompt = '''def print_prime(n):\n",
    "       \"\"\"\n",
    "       Print all primes between 1 and n\n",
    "       \"\"\"'''\n",
    "    return generate(model, tokenizer, prompt, display)\n",
    "\n",
    "def run_coding_sample_2(model, tokenizer):\n",
    "    prompt = '''def array_sum(array, nrows, ncols):\n",
    "       \"\"\"\n",
    "       Use two for loops to add elements of an array.\n",
    "       \"\"\"'''\n",
    "    return generate(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Baseline"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> George Washington.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 31 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> George Washington.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 29 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'>...\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 27 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'>...\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 25 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> [insert name].\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 23 to 30"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> John.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 21 to 28"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Instruct: Who was the first United States president?\n",
       "\n",
       "Output: Their name was</pre><pre style='background-color: rgb(200, 255, 200, 1.0)'> George Washington.\n",
       "<|endoftext|><pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hooked_phi import attach_hooks, detach_hooks\n",
    "\n",
    "all_results = []\n",
    "all_results_tokenized = []\n",
    "\n",
    "# Returns a hook that can be used to ablate a set of neurons.\n",
    "def ablate_neurons(mask):\n",
    "    assert mask.shape[0] == model.config.num_hidden_layers\n",
    "    assert mask.shape[1] == model.config.intermediate_size\n",
    "\n",
    "    def hook(neurons, layer_idx):\n",
    "        neurons[..., ~mask[layer_idx]] = 0\n",
    "        return neurons\n",
    "\n",
    "    return hook\n",
    "\n",
    "# Ablate the last layer MLP.\n",
    "end_layer = 32\n",
    "for num_ablation_layers in range(0, 13, 2):\n",
    "    start_layer = 32 - num_ablation_layers\n",
    "    end_layer = min(32 - num_ablation_layers + 8, 32)\n",
    "    \n",
    "    if num_ablation_layers == 0:\n",
    "        display(Markdown(f'## Baseline'))\n",
    "    else:\n",
    "        display(Markdown(f'## Ablating MLPs {32 - num_ablation_layers + 1} to {end_layer}'))\n",
    "    \n",
    "    mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    if num_ablation_layers > 0:\n",
    "        mask[start_layer:end_layer, :] = False\n",
    "    \n",
    "    attach_hooks(model.model, ablate_neurons(mask))\n",
    "    \n",
    "    # input_ids, input_and_completion_ids, result = run_coding_sample(model, tokenizer)\n",
    "    input_ids, input_and_completion_ids, result = generate(model, tokenizer, \"Instruct: Who was the first United States president?\\n\\nOutput: Their name was\", do_display=True)\n",
    "    if num_ablation_layers == 0:\n",
    "        ground_truth = result\n",
    "    \n",
    "    all_results.append(result)\n",
    "\n",
    "detach_hooks(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from evaluator import perplexity_evaluator\n",
    "\n",
    "perplexity_result = perplexity_evaluator(preds=all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog is running with a ball in its mouth. the dog is running towards a person. the person is smiling and waving at the dog. the text says \"Meet Max, the best dog in the world. He loves to play fetch and make new friends', '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy. [substeps] Cognitive behavioral therapy (CBT) is a common approach that helps people identify and change negative thought patterns. Exposure therapy is another approach that involves gradually exposing people to their fears in a safe and controlled environment. [substeps] Med', '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits. Instead, try to remain calm and avoid reacting defensively. [substeps]\\n#*For example, if someone says, “I’m not mad at you,” you might respond, “I’m not mad', '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments. [step] The higher the nrr, the more effective the hearing protection device is. [step] The nrr is measured in decibels (db) and is expressed as a number. For example, a hearing protection device with an n', 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he is wearing a white shirt and black pants. He is holding a tool in his hand.\\n\\nQuestion 2:\\nThe man is using a tool to hit something on the table before sanding it. What is the tool?\\n\\nAnswer 2']\n"
     ]
    }
   ],
   "source": [
    "from evaluator import charcut_evaluator\n",
    "\n",
    "ground_truths = []\n",
    "predictions = {}\n",
    "\n",
    "for prompt in selected_prompts:\n",
    "    \n",
    "    # Ablate the last layer MLP.\n",
    "    for num_ablation_layers in range(0, 5, 2):\n",
    "        \n",
    "        mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "        if num_ablation_layers > 0:\n",
    "            mask[-num_ablation_layers:, :] = False\n",
    "        \n",
    "        attach_hooks(model.model, ablate_neurons(mask))\n",
    "        \n",
    "        input_ids, input_and_completion_ids, result = generate(model, tokenizer, prompt, do_display=False)\n",
    "        \n",
    "        if num_ablation_layers == 0:\n",
    "            ground_truths.append(prompt+result)\n",
    "        \n",
    "        if num_ablation_layers not in predictions:\n",
    "            predictions[num_ablation_layers] = []\n",
    "        \n",
    "        predictions[num_ablation_layers].append(prompt+result)\n",
    "        \n",
    "\n",
    "    detach_hooks(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A text intro leads into a picture of a dog and the same dog running along the yard. the dog is running with a ball in its mouth. the dog is running towards a person. the person is smiling and waving at the dog. the text says \"Meet Max, the best dog in the world. He loves to play fetch and make new friends', '[header] How to overcome fear of disease [title] Work with a therapist. [step] Therapy is generally considered one of the most effective ways to manage anxiety disorders, and illness anxiety disorder is no different. There are many different approaches to therapy. [substeps] Cognitive behavioral therapy (CBT) is a common approach that helps people identify and change negative thought patterns. Exposure therapy is another approach that involves gradually exposing people to their fears in a safe and controlled environment. [substeps] Med', '[header] How to respond to passive aggressive comments [title] Avoid reacting defensively. [step] When someone makes a passive aggressive comment, you may feel the need to defend yourself, or make accusations about them. Getting upset will likely do little good to change their habits. Instead, try to remain calm and avoid reacting defensively. [substeps]\\n#*For example, if someone says, “I’m not mad at you,” you might respond, “I’m not mad', '[header] How to select hearing protection [title] Familiarize yourself with noise reduction ratings (nrr). [step] Nrr is the standard rating system across all hearing protection devices. Under nrr, hearing devices are classified by their potential to limit decibels (db) in professional and occupational environments. [step] The higher the nrr, the more effective the hearing protection device is. [step] The nrr is measured in decibels (db) and is expressed as a number. For example, a hearing protection device with an n', 'He uses a tool to hit something on the table before sanding it. The man pulls off his gloves while walking to the camera. he is wearing a white shirt and black pants. He is holding a tool in his hand.\\n\\nQuestion 2:\\nThe man is using a tool to hit something on the table before sanding it. What is the tool?\\n\\nAnswer 2']\n"
     ]
    }
   ],
   "source": [
    "print(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for key, value in predictions.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charcut_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Visualizing How Log-Likelihood Evolves, and For Which Tokens"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Baseline"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: 0.0\n",
      "Worst deflection: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Instruct</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Who</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;the</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;first</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;United</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;States</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;president</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>?</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Output</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Their</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;name</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;George</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Washington</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>.</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 32 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: 0.023425403982400894\n",
      "Worst deflection: -0.00902162492275238\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Instruct</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Who</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;the</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;first</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;United</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;States</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;president</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>?</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Output</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Their</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;name</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240, 237.83480834960938, 237.83480834960938)'>&nbsp;George</span><span class='token' style='background-color:rgb(240, 239.97564697265625, 239.97564697265625)'>&nbsp;Washington</span><span class='token' style='background-color:rgb(220.9441680908203, 240, 220.9441680908203)'>.</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 31 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.02048165537416935\n",
      "Worst deflection: -0.06030045077204704\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Instruct</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Who</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;the</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;first</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;United</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;States</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;president</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>?</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Output</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Their</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;name</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240, 225.52789306640625, 225.52789306640625)'>&nbsp;George</span><span class='token' style='background-color:rgb(239.90817260742188, 240, 239.90817260742188)'>&nbsp;Washington</span><span class='token' style='background-color:rgb(240, 239.63348388671875, 239.63348388671875)'>.</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 30 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.2060624361038208\n",
      "Worst deflection: -0.480381578207016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Instruct</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Who</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;the</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;first</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;United</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;States</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;president</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>?</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Output</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Their</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;name</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240, 124.70841217041016, 124.70841217041016)'>&nbsp;George</span><span class='token' style='background-color:rgb(240, 239.53366088867188, 239.53366088867188)'>&nbsp;Washington</span><span class='token' style='background-color:rgb(240, 207.39297485351562, 207.39297485351562)'>.</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Ablating MLPs 29 to 32"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average deflection: -0.9148145914077759\n",
      "Worst deflection: -2.6101691722869873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Instruct</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Who</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;the</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;first</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;United</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;States</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;president</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>?</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>\n",
       "</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>Output</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>:</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;Their</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;name</span><span class='token' style='background-color:rgb(240.0, 240, 240.0)'>&nbsp;was</span><span class='token' style='background-color:rgb(240, 0, 0)'>&nbsp;George</span><span class='token' style='background-color:rgb(240, 217.66329956054688, 217.66329956054688)'>&nbsp;Washington</span><span class='token' style='background-color:rgb(240, 230.11082458496094, 230.11082458496094)'>.</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is called \"neuron vis\", but can really be used for anything involving a score assigned to each token.\n",
    "# In this case, visualize logprob\n",
    "from neuron_visualization import basic_neuron_vis, basic_neuron_vis_signed\n",
    "\n",
    "# Get granular nll estimates\n",
    "display(Markdown(f'# Visualizing How Log-Likelihood Evolves, and For Which Tokens'))\n",
    "\n",
    "# 1. Get ground truth.\n",
    "detach_hooks(model.model)\n",
    "input_ids, input_and_completion_ids, ground_truth = generate(model, tokenizer, \"Instruct: Who was the first United States president?\\n\\nOutput: Their name was\", do_display=False, max_new_tokens=3)\n",
    "completion_ids = input_and_completion_ids[len(input_ids):]\n",
    "\n",
    "# Calculate baseline logprobs.\n",
    "predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "logits = predictions_for_next_token[0][0]\n",
    "logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "baseline_logprobs_for_sampled_output_tokens = logprobs_for_output_tokens[\n",
    "    torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "    completion_ids\n",
    "]\n",
    "\n",
    "# 2. Ablate model. See which suddens suddenly become highly unlikely (by visualizing negative logprobs).\n",
    "# for ablation_layer in range(31, 31 - 8 - 1, -1):\n",
    "# # for ablation_layer in range(31, -1, -1):\n",
    "#     mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    \n",
    "#     display(Markdown(f'## Ablating MLP {ablation_layer + 1} alone'))\n",
    "#     mask[ablation_layer, :] = False\n",
    "\n",
    "for num_ablation_layers in range(0, 5, 1):\n",
    "    mask = torch.ones((model.config.num_hidden_layers, model.config.intermediate_size), dtype=torch.bool)\n",
    "    if num_ablation_layers == 0:\n",
    "        display(Markdown(f'## Baseline'))\n",
    "    else:\n",
    "        display(Markdown(f'## Ablating MLPs {32 - num_ablation_layers + 1} to 32'))\n",
    "    if num_ablation_layers > 0:\n",
    "        mask[-num_ablation_layers:, :] = False\n",
    "\n",
    "    attach_hooks(model.model, ablate_neurons(mask))\n",
    "    predictions_for_next_token = model(input_ids=input_and_completion_ids.unsqueeze(0))\n",
    "    logits = predictions_for_next_token[0][0]\n",
    "    logits_for_output_tokens = logits[len(input_ids) - 1:-1]\n",
    "    logprobs_for_output_tokens = torch.log_softmax(logits_for_output_tokens, dim=-1)\n",
    "    logprobs_for_sampled_output_tokens = logprobs_for_output_tokens[\n",
    "        torch.arange(logprobs_for_output_tokens.shape[0]),\n",
    "        completion_ids\n",
    "    ]\n",
    "\n",
    "    # visualized_tokens = outputs[0].cpu()[len(inputs['input_ids'][0].cpu()):]\n",
    "    visualized_tokens = input_and_completion_ids.cpu()\n",
    "    token_names = [\n",
    "        tokenizer.decode(torch.tensor([visualized_tokens[i]]))\n",
    "        for i in range(len(visualized_tokens))\n",
    "    ]\n",
    "\n",
    "    deflections = logprobs_for_sampled_output_tokens - baseline_logprobs_for_sampled_output_tokens\n",
    "\n",
    "    colors = torch.cat([torch.zeros_like(input_ids), deflections])\n",
    "\n",
    "    print(\"Average deflection:\", deflections.mean().item())\n",
    "    print(\"Worst deflection:\", deflections.min().item())\n",
    "    html = basic_neuron_vis_signed(token_names, colors, 1)\n",
    "    display(HTML(html))\n",
    "\n",
    "detach_hooks(model.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing \"kernel\" of MLP intermediate states\n",
    "\n",
    "If there MLP intermediate states represent recall coefficients - with the MLP encoder representing detection, and the MLP decoder representing triggering of the new memory - we would expect that most MLP intermediate states would be 0 (as only a sparse number of \"memories\" should be activated at any given time), or that MLP intermediate states would be correlated with meaningful concepts. Let's test this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_neurons(neurons, layer_idx):\n",
    "    if layer_idx == 0:\n",
    "        log.append([])\n",
    "\n",
    "    log[-1].append(neurons)\n",
    "    \n",
    "    return neurons\n",
    "\n",
    "attach_hooks(model.model, log_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "inputs, outputs, text = run_coding_sample_2(model, tokenizer)\n",
    "\n",
    "layer_id = 0\n",
    "\n",
    "all_mlp_activations = []\n",
    "for i in range(len(log)):\n",
    "    # log[forward pass index][layer index][batch index, token index in forward pass, mlp neuron index]\n",
    "    mlp_activations = log[i][layer_id][0, -1, :]\n",
    "    all_mlp_activations.append(mlp_activations)\n",
    "\n",
    "stacked = torch.stack(all_mlp_activations, dim=0)\n",
    "\n",
    "# Scale each MLP intermediate neuron by the maximum absolute value\n",
    "scale_by_dim = torch.max(stacked.abs(), dim=0, keepdims=True).values\n",
    "stacked_scaled = stacked / (scale_by_dim + 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"MLP activations: Last Layer (Unscaled)\")\n",
    "plt.hist(stacked.view(-1).cpu(), bins=25)\n",
    "plt.xlabel(\"MLP activation level\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"MLP activations: Last Layer (Scaled per dim.)\")\n",
    "plt.hist(stacked_scaled.view(-1).cpu(), bins=25)\n",
    "plt.xlabel(\"MLP activation level\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "It seems that MLP activations are very sparse. Therefore, it will hopefully be relatively simple to find meaningful MLP neurons to visualize.\n",
    "\n",
    "To automatically determine which MLP activations are most interesting, I will calculate the variance of the activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuron_visualization import basic_neuron_vis\n",
    "\n",
    "variance = stacked.var(dim=0)\n",
    "highest_to_lowest_variance = variance.argsort(descending=True)\n",
    "visualized_tokens = outputs[0].cpu()[len(inputs['input_ids'][0].cpu()):]\n",
    "\n",
    "token_names = [\n",
    "    tokenizer.decode(torch.tensor([visualized_tokens[i]]))\n",
    "    for i in range(len(visualized_tokens))\n",
    "]\n",
    "\n",
    "visualized_neurons = highest_to_lowest_variance[:100]\n",
    "\n",
    "for neuron_i in range(len(visualized_neurons)):\n",
    "    neuron_id = visualized_neurons[neuron_i]\n",
    "    html = basic_neuron_vis(token_names, stacked[:, neuron_id], layer=layer_id, neuron_index=neuron_id)\n",
    "    display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
