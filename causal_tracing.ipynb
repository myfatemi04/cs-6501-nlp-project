{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = '/scratch/gsk6me/huggingface_cache/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is gpt2-small\n",
    "import transformers\n",
    "model = transformers.GPT2Model.from_pretrained(\"openai-community/gpt2\").cuda()\n",
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 3072]), torch.Size([3072, 768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].mlp.c_fc.weight.shape, model.h[0].mlp.c_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
      "        hidden_states = self.c_fc(hidden_states)\n",
      "        hidden_states = self.act(hidden_states)\n",
      "        hidden_states = self.c_proj(hidden_states)\n",
      "        hidden_states = self.dropout(hidden_states)\n",
      "        return hidden_states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "# seems pretty straightforward\n",
    "print(inspect.getsource(model.h[0].mlp.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0\n",
      "Loading 1\n",
      "Loading 2\n",
      "Loading 3\n",
      "Loading 4\n",
      "Loading 5\n",
      "Loading 6\n",
      "Loading 7\n",
      "Loading 8\n",
      "Loading 9\n",
      "Loading 10\n",
      "Loading 11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import blobfile as bf\n",
    "import transformer_lens\n",
    "import sparse_autoencoder\n",
    "\n",
    "# Load the autoencoders\n",
    "autoencoders = []\n",
    "\n",
    "for layer_index in range(12):\n",
    "    print(\"Loading\", layer_index)\n",
    "    autoencoder_input = [\"mlp_post_act\", \"resid_delta_mlp\"][0]\n",
    "    filename = f\"az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt\"\n",
    "    with bf.BlobFile(filename, mode=\"rb\", streaming=False, cache_dir='/scratch/gsk6me/sae-gpt2-small-cache') as f:\n",
    "        state_dict = torch.load(f)\n",
    "    autoencoder = sparse_autoencoder.Autoencoder.from_state_dict(state_dict)\n",
    "    autoencoders.append(autoencoder.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "\n",
    "patch_layer = 0\n",
    "patch_seqpos = 0\n",
    "patch_index = 0\n",
    "capture_layer = 0\n",
    "capture_index = 0\n",
    "\n",
    "patch_history = []\n",
    "activation_history = []\n",
    "\n",
    "def custom_fwd(self, hidden_states, layer_num):\n",
    "    global patch_index, patch_layer, capture_index, patch_history, activation_history\n",
    "    # shape: [batch_size, num_tokens, hidden_shape]\n",
    "    hidden_states = self.c_fc(hidden_states)\n",
    "    hidden_states = self.act(hidden_states)    \n",
    "    # patch the specific feature index (incl. both sequence number and feature id)\n",
    "    if patch_layer == layer_num:\n",
    "        hidden_states = F.relu(\n",
    "            autoencoders[layer_num].encoder(hidden_states)\n",
    "        )\n",
    "        hidden_states[:, :, patch_index] = 0\n",
    "        hidden_states = autoencoders[layer_num].decoder(hidden_states)\n",
    "        patch_history.append((patch_layer, patch_seqpos, patch_index))\n",
    "        \n",
    "    # capture the change in sparse feature (will likely be applied at a later layer\n",
    "    # as the earlier if statement)\n",
    "    if capture_layer == layer_num:\n",
    "        features = F.relu(\n",
    "            autoencoders[layer_num].encoder(hidden_states)\n",
    "        )\n",
    "        activation = features[:, -1, capture_index]\n",
    "        activation_history.append(activation)\n",
    "    elif capture_layer == -1:\n",
    "        # capture everything\n",
    "        features = F.relu(\n",
    "            autoencoders[layer_num].encoder(hidden_states)\n",
    "        )\n",
    "        activation_history[-1][layer_num] = features\n",
    "    hidden_states = self.c_proj(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "for i in range(12):\n",
    "    mlp = model.h[i].mlp\n",
    "    mlp.forward = partial(custom_fwd.__get__(mlp, type(mlp)), layer_num=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0082, 0.0062]], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "2 0.8764863014221191\n",
      "3 3.9374032020568848\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# estimate performance diff\n",
    "\n",
    "def get_patching_results(string, cap_layer, cap_index):\n",
    "    \"\"\"\n",
    "    gets activation patching results for a specific feature\n",
    "    \"\"\"\n",
    "    # get baseline\n",
    "    global patch_index, patch_layer, patch_history, capture_layer, capture_index, activation_history\n",
    "    \n",
    "    patch_history = []\n",
    "    activation_history = []\n",
    "    \n",
    "    patch_index = 0\n",
    "    patch_layer = -1\n",
    "    capture_layer = -1\n",
    "    patch_history.append(None)\n",
    "    activation_history.append({})\n",
    "    baseline = model(**tokenizer(string, return_tensors='pt').to('cuda'))\n",
    "    \n",
    "    print(activation_history[0][cap_layer][:, :, cap_index])\n",
    "    \n",
    "    capture_layer = cap_layer\n",
    "    capture_index = cap_index\n",
    "    \n",
    "    for layer_i in range(2, 12):\n",
    "        h = activation_history[0][layer_i]\n",
    "        h = torch.max(h[0], dim=0).values\n",
    "        h[h < 1e-1] = 0\n",
    "        start = time.time()\n",
    "        for feat in h.nonzero():\n",
    "            patch_layer = layer_i\n",
    "            patch_index = feat[0].item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model(**tokenizer(string, return_tensors=\"pt\").to('cuda'))\n",
    "        end = time.time()\n",
    "        print(layer_i, end-start)\n",
    "                \n",
    "        if layer_i > cap_layer:\n",
    "            break\n",
    "\n",
    "get_patching_results(\"hello!\", 2, 3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse",
   "language": "python",
   "name": "sparse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
